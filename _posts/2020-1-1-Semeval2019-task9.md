---
layout: post
title: Evaluation of models used in Semeval 2019
---

# Semeval 2019 - Task 9 - Suggestion mining
* 7 out of the top 13 teams used BERT
* Subtasks A,B came from different distributions

## Teams
### OleNet 
* [Code](https://git.coding-space.cn/guange/models/src/ecd723a18f31ed12b31b1cc07e67db8ea110b8e5/PaddleNLP/Research/NAACL2019-MPM)
* [Paper](https://www.aclweb.org/anthology/S19-2216.pdf)
* Ranked 1, 2
* Used BERT in multiple ways:
- The same ordinary way of a binary classification problem (logistic network on top of first vector)
- Add two CNN layer on top of the BERT Embeddings. Each layer is succeeded by Batch Normalization.
Max pooling is also used. For the activation, RelU is used.
- Used a GRU layer
- Used an attention layer called FFA! (https://arxiv.org/pdf/1512.08756.pdf)
* Found that the model relied on specific words in classification.
Thus the top (J=100) words were dropped from the sentence with a probability (alpha= 0.5) as a data augmentation procedure.

### NTUA-ISLab
* [Paper](https://www.aclweb.org/anthology/S19-2215.pdf)
* Ranked 5, 1
* Used a set of patterns üòç and an RNN
* The classification result coming from the R-CNN network couples the rule-based classification (presented in Section 4.2) following a parallel fail-safe mode, that is: in the event that the softmax output exceeds a certain confidence threshold limit for a specific class label and the rule-based classification induces the opposite label, the finally assigned class label is the one induced by the R-CNN.

### MIDAS
* Ranked 10, NA
* Used oversampling to solve class imbalance
* Used ULMfit and seems like BERT is more powerful
